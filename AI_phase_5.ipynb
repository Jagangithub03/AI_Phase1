{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d5c659c096cadc28f2cdd65abfb7a61fc0777c8"
   },
   "source": [
    "Here, we used the RandomForestRegressor model to predict the outputs, we see the strange prediction from this with score above 80% which can be assumed to be best fit but not due to its predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "b6f9808731d449921637b96b9a2aef2961239a0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  5.96,  50.97],\n",
       "       [  5.88,  37.8 ],\n",
       "       [  5.97,  37.6 ],\n",
       "       ...,\n",
       "       [  6.42,  19.9 ],\n",
       "       [  5.73, 591.55],\n",
       "       [  5.68,  33.61]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = RandomForestRegressor(random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "a2b8480ab26f551e16e45d3c70cda8192a402c9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8614799631765803"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "682a29a27dbbaf14303a8881f7fc13aacbc5b309"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.8888 ,  43.532  ],\n",
       "       [  5.8232 ,  31.71656],\n",
       "       [  6.0034 ,  39.3312 ],\n",
       "       ...,\n",
       "       [  6.3066 ,  23.9292 ],\n",
       "       [  5.9138 , 592.151  ],\n",
       "       [  5.7866 ,  38.9384 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators':[10, 20, 50, 100, 200, 500]}\n",
    "\n",
    "grid_obj = GridSearchCV(reg, parameters)\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "best_fit = grid_fit.best_estimator_\n",
    "best_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "35d6cf6059a072189e9f95dad28f5cc0d8b9a7db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8749008584467053"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "215d5185f6964cf0ffbf313d6d5e285e9ad873db"
   },
   "source": [
    "### Neural Network model\n",
    "In the above case it was more kind of linear regressor where the predicted values are not as expected. So, Now, we build the neural network to fit the data for training set. Neural Network consists of three Dense layer with each 16, 16, 2 nodes and relu, relu and softmax as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ffcfb0d29010f5addab990783abab0923f7a1dcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def create_model(neurons, activation, optimizer, loss):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation, input_shape=(3,)))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b053784767ceb4eb8171781f4667f4d296c4b197"
   },
   "source": [
    "In this, we define the hyperparameters with two or more options to find the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "37e01b437bf48b4542a1cde0f9ece0ee427d7d6d"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# neurons = [16, 64, 128, 256]\n",
    "neurons = [16]\n",
    "# batch_size = [10, 20, 50, 100]\n",
    "batch_size = [10]\n",
    "epochs = [10]\n",
    "# activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', 'exponential']\n",
    "activation = ['sigmoid', 'relu']\n",
    "# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "optimizer = ['SGD', 'Adadelta']\n",
    "loss = ['squared_hinge']\n",
    "\n",
    "param_grid = dict(neurons=neurons, batch_size=batch_size, epochs=epochs, activation=activation, optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "363f1a26772010cc10044357fedc2d8330cc7722"
   },
   "source": [
    "Here, we find the best fit of the above model and get the mean test score and standard deviation of the best fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "e95a9220f51b2483cb581502a00e8aef484ad6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.666684 using {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'SGD'}\n",
      "0.666684 (0.471398) with: {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'SGD'}\n",
      "0.000000 (0.000000) with: {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'Adadelta'}\n",
      "0.666684 (0.471398) with: {'activation': 'relu', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'SGD'}\n",
      "0.000000 (0.000000) with: {'activation': 'relu', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'Adadelta'}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af169ec22446a617bff56c1754e666f0bbb41815"
   },
   "source": [
    "The best fit parameters are used for same model to compute the score with training data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "6d43c955e2dac981b7858021de173ceb8b09e461"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(3,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD', loss='squared_hinge', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "8d75707cb8c906834c485c805979fe3836a6ad6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18727 samples, validate on 4682 samples\n",
      "Epoch 1/20\n",
      "18727/18727 [==============================] - 4s 233us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 2/20\n",
      "18727/18727 [==============================] - 4s 220us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 3/20\n",
      "18727/18727 [==============================] - 4s 228us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 4/20\n",
      "18727/18727 [==============================] - 4s 222us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 5/20\n",
      "18727/18727 [==============================] - 5s 262us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 6/20\n",
      "18727/18727 [==============================] - 4s 223us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 7/20\n",
      "18727/18727 [==============================] - 4s 220us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 8/20\n",
      "18727/18727 [==============================] - 4s 224us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 9/20\n",
      "18727/18727 [==============================] - 4s 220us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 10/20\n",
      "18727/18727 [==============================] - 4s 224us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 11/20\n",
      "18727/18727 [==============================] - 4s 221us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 12/20\n",
      "18727/18727 [==============================] - 4s 231us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 13/20\n",
      "18727/18727 [==============================] - 5s 248us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 14/20\n",
      "18727/18727 [==============================] - 4s 220us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 15/20\n",
      "18727/18727 [==============================] - 4s 223us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 16/20\n",
      "18727/18727 [==============================] - 4s 222us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 17/20\n",
      "18727/18727 [==============================] - 4s 225us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 18/20\n",
      "18727/18727 [==============================] - 4s 219us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 19/20\n",
      "18727/18727 [==============================] - 4s 220us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n",
      "Epoch 20/20\n",
      "18727/18727 [==============================] - 5s 258us/step - loss: 0.5038 - acc: 0.9182 - val_loss: 0.5038 - val_acc: 0.9242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x78dfa2107ef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "95252c11f03124f97ffeb951dd1835aa7e29ee22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4682/4682 [==============================] - 0s 29us/step\n",
      "Evaluation result on Test Data : Loss = 0.5038455790406056, accuracy = 0.9241777017858995\n"
     ]
    }
   ],
   "source": [
    "[test_loss, test_acc] = model.evaluate(X_test, y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f462dff246920441300275da5591b6cf3536e03"
   },
   "source": [
    "We see that the above model performs better but it also has lot of noise (loss) which can be neglected for prediction and use it for furthur prediction.\n",
    "\n",
    "The above model is saved for furthur prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "56147c5973b579388f146c2da81e721ad4724b03"
   },
   "outputs": [],
   "source": [
    "model.save('earthquake.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
